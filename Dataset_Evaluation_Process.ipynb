{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d58d9d",
   "metadata": {},
   "source": [
    "# Dataset Evaluation Process\n",
    "## Systematic Selection for Data Science Portfolio\n",
    "\n",
    "**Date:** June 28, 2025  \n",
    "**Purpose:** Evaluate 6 available datasets to select the optimal one for portfolio development  \n",
    "**Methodology:** Based on Data Quality (40%), Business Relevance (35%), and Technical Complexity (25%)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Evaluation Objective\n",
    "\n",
    "This notebook systematically evaluates all available datasets to identify which one best demonstrates data science skills while providing meaningful business insights for potential employers.\n",
    "\n",
    "### Evaluation Criteria:\n",
    "1. **Data Quality Assessment** (40% weight)\n",
    "2. **Business Relevance Evaluation** (35% weight)  \n",
    "3. **Technical Complexity & Skill Showcase** (25% weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec331018",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll import pandas for data manipulation, numpy for numerical operations, and other libraries needed for our comprehensive dataset evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "958e848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.0\n",
      "NumPy version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for dataset evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings for better output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583030b9",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Each Dataset\n",
    "\n",
    "Let's systematically load each dataset and perform initial inspection to understand the structure, size, and basic characteristics.\n",
    "\n",
    "### Available Datasets:\n",
    "1. **Airbnb** (airbnb.xlsx) - Real estate/hospitality data\n",
    "2. **Netflix** (netflix_titles.xlsx) - Entertainment/content data\n",
    "3. **Superstore** (sample_-_superstore.xls) - Retail/sales data\n",
    "4. **Spotify** (SpotifyFeatures.csv) - Music/audio analytics data\n",
    "5. **Titanic** (titanic passenger list.csv) - Historical/demographic data\n",
    "6. **AI Adoption** (ai_adoption_dataset.csv) - Technology adoption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee611c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading all datasets...\n",
      "\n",
      "============================================================\n",
      "DATASET: AIRBNB\n",
      "============================================================\n",
      "üìÅ File: airbnb.xlsx\n",
      "üìä Shape: 30,478 rows √ó 13 columns\n",
      "üíæ Memory usage: 9.58 MB\n",
      "\n",
      "üìã Column Names:\n",
      "   1. Host Id\n",
      "   2. Host Since\n",
      "   3. Name\n",
      "   4. Neighbourhood \n",
      "   5. Property Type\n",
      "   6. Review Scores Rating (bin)\n",
      "   7. Room Type\n",
      "   8. Zipcode\n",
      "   9. Beds\n",
      "  10. Number of Records\n",
      "  11. Number Of Reviews\n",
      "  12. Price\n",
      "  13. Review Scores Rating\n",
      "\n",
      "üîç Data Types:\n",
      "int64             4\n",
      "object            4\n",
      "float64           4\n",
      "datetime64[ns]    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìà Sample Data (First 3 rows):\n",
      "    Host Id Host Since                             Name Neighbourhood   \\\n",
      "0   5162530        NaT  1 Bedroom in Prime Williamsburg       Brooklyn   \n",
      "1  33134899        NaT  Sunny, Private room in Bushwick       Brooklyn   \n",
      "2  39608626        NaT             Sunny Room in Harlem      Manhattan   \n",
      "\n",
      "  Property Type  Review Scores Rating (bin)        Room Type  Zipcode  Beds  \\\n",
      "0     Apartment                         NaN  Entire home/apt  11249.0   1.0   \n",
      "1     Apartment                         NaN     Private room  11206.0   1.0   \n",
      "2     Apartment                         NaN     Private room  10032.0   1.0   \n",
      "\n",
      "   Number of Records  Number Of Reviews  Price  Review Scores Rating  \n",
      "0                  1                  0    145                   NaN  \n",
      "1                  1                  1     37                   NaN  \n",
      "2                  1                  1     28                   NaN  \n",
      "\n",
      "============================================================\n",
      "DATASET: NETFLIX\n",
      "============================================================\n",
      "üìÅ File: netflix_titles.xlsx\n",
      "üìä Shape: 6,236 rows √ó 9 columns\n",
      "üíæ Memory usage: 3.12 MB\n",
      "\n",
      "üìã Column Names:\n",
      "   1. duration_minutes\n",
      "   2. duration_seasons\n",
      "   3. type\n",
      "   4. title\n",
      "   5. date_added\n",
      "   6. release_year\n",
      "   7. rating\n",
      "   8. description\n",
      "   9. show_id\n",
      "\n",
      "üîç Data Types:\n",
      "object     7\n",
      "float64    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìà Sample Data (First 3 rows):\n",
      "  duration_minutes duration_seasons     type  \\\n",
      "0               90              NaN    Movie   \n",
      "1               94              NaN    Movie   \n",
      "2              NaN                1  TV Show   \n",
      "\n",
      "                                     title           date_added  release_year  \\\n",
      "0  Norm of the North: King Sized Adventure  2019-09-09 00:00:00        2019.0   \n",
      "1               Jandino: Whatever it Takes  2016-09-09 00:00:00        2016.0   \n",
      "2                       Transformers Prime  2018-09-08 00:00:00        2013.0   \n",
      "\n",
      "     rating                                        description     show_id  \n",
      "0     TV-PG  Before planning an awesome wedding for his gra...  81145628.0  \n",
      "1     TV-MA  Jandino Asporaat riffs on the challenges of ra...  80117401.0  \n",
      "2  TV-Y7-FV  With the help of three human allies, the Autob...  70234439.0  \n",
      "\n",
      "============================================================\n",
      "DATASET: SUPERSTORE\n",
      "============================================================\n",
      "üìÅ File: sample_-_superstore.xls\n",
      "üìä Shape: 9,994 rows √ó 21 columns\n",
      "üíæ Memory usage: 8.25 MB\n",
      "\n",
      "üìã Column Names:\n",
      "   1. Row ID\n",
      "   2. Order ID\n",
      "   3. Order Date\n",
      "   4. Ship Date\n",
      "   5. Ship Mode\n",
      "   6. Customer ID\n",
      "   7. Customer Name\n",
      "   8. Segment\n",
      "   9. Country\n",
      "  10. City\n",
      "  11. State\n",
      "  12. Postal Code\n",
      "  13. Region\n",
      "  14. Product ID\n",
      "  15. Category\n",
      "  16. Sub-Category\n",
      "  17. Product Name\n",
      "  18. Sales\n",
      "  19. Quantity\n",
      "  20. Discount\n",
      "  21. Profit\n",
      "\n",
      "üîç Data Types:\n",
      "object            13\n",
      "int64              3\n",
      "float64            3\n",
      "datetime64[ns]     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìà Sample Data (First 3 rows):\n",
      "   Row ID        Order ID Order Date  Ship Date     Ship Mode Customer ID  \\\n",
      "0       1  CA-2016-152156 2016-11-08 2016-11-11  Second Class    CG-12520   \n",
      "1       2  CA-2016-152156 2016-11-08 2016-11-11  Second Class    CG-12520   \n",
      "2       3  CA-2016-138688 2016-06-12 2016-06-16  Second Class    DV-13045   \n",
      "\n",
      "     Customer Name    Segment        Country         City       State  \\\n",
      "0      Claire Gute   Consumer  United States    Henderson    Kentucky   \n",
      "1      Claire Gute   Consumer  United States    Henderson    Kentucky   \n",
      "2  Darrin Van Huff  Corporate  United States  Los Angeles  California   \n",
      "\n",
      "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
      "0        42420  South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1        42420  South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2        90036   West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "\n",
      "                                        Product Name   Sales  Quantity  \\\n",
      "0                  Bush Somerset Collection Bookcase  261.96         2   \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.94         3   \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.62         2   \n",
      "\n",
      "   Discount    Profit  \n",
      "0       0.0   41.9136  \n",
      "1       0.0  219.5820  \n",
      "2       0.0    6.8714  \n",
      "\n",
      "============================================================\n",
      "DATASET: SPOTIFY\n",
      "============================================================\n",
      "üìÅ File: SpotifyFeatures.csv\n",
      "üìä Shape: 232,725 rows √ó 18 columns\n",
      "üíæ Memory usage: 111.81 MB\n",
      "\n",
      "üìã Column Names:\n",
      "   1. genre\n",
      "   2. artist_name\n",
      "   3. track_name\n",
      "   4. track_id\n",
      "   5. popularity\n",
      "   6. acousticness\n",
      "   7. danceability\n",
      "   8. duration_ms\n",
      "   9. energy\n",
      "  10. instrumentalness\n",
      "  11. key\n",
      "  12. liveness\n",
      "  13. loudness\n",
      "  14. mode\n",
      "  15. speechiness\n",
      "  16. tempo\n",
      "  17. time_signature\n",
      "  18. valence\n",
      "\n",
      "üîç Data Types:\n",
      "float64    9\n",
      "object     7\n",
      "int64      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìà Sample Data (First 3 rows):\n",
      "   genre        artist_name                        track_name  \\\n",
      "0  Movie     Henri Salvador       C'est beau de faire un Show   \n",
      "1  Movie  Martin & les f√©es  Perdu d'avance (par Gad Elmaleh)   \n",
      "2  Movie    Joseph Williams    Don't Let Me Be Lonely Tonight   \n",
      "\n",
      "                 track_id  popularity  acousticness  danceability  \\\n",
      "0  0BRjO6ga9RKCKjfDqeFgWV           0         0.611         0.389   \n",
      "1  0BjC1NfoEOOusryehmNudP           1         0.246         0.590   \n",
      "2  0CoSDzoNIKCRs124s9uTVy           3         0.952         0.663   \n",
      "\n",
      "   duration_ms  energy  instrumentalness key  liveness  loudness   mode  \\\n",
      "0        99373   0.910               0.0  C#     0.346    -1.828  Major   \n",
      "1       137373   0.737               0.0  F#     0.151    -5.559  Minor   \n",
      "2       170267   0.131               0.0   C     0.103   -13.879  Minor   \n",
      "\n",
      "   speechiness    tempo time_signature  valence  \n",
      "0       0.0525  166.969            4/4    0.814  \n",
      "1       0.0868  174.003            4/4    0.816  \n",
      "2       0.0362   99.488            5/4    0.368  \n",
      "\n",
      "============================================================\n",
      "DATASET: TITANIC\n",
      "============================================================\n",
      "üìÅ File: titanic passenger list.csv\n",
      "üìä Shape: 1,309 rows √ó 14 columns\n",
      "üíæ Memory usage: 0.52 MB\n",
      "\n",
      "üìã Column Names:\n",
      "   1. pclass\n",
      "   2. survived\n",
      "   3. name\n",
      "   4. sex\n",
      "   5. age\n",
      "   6. sibsp\n",
      "   7. parch\n",
      "   8. ticket\n",
      "   9. fare\n",
      "  10. cabin\n",
      "  11. embarked\n",
      "  12. boat\n",
      "  13. body\n",
      "  14. home.dest\n",
      "\n",
      "üîç Data Types:\n",
      "object     7\n",
      "int64      4\n",
      "float64    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìà Sample Data (First 3 rows):\n",
      "   pclass  survived                            name     sex    age  sibsp  \\\n",
      "0       1         1   Allen, Miss. Elisabeth Walton  female  29.00      0   \n",
      "1       1         1  Allison, Master. Hudson Trevor    male   0.92      1   \n",
      "2       1         0    Allison, Miss. Helen Loraine  female   2.00      1   \n",
      "\n",
      "   parch  ticket      fare    cabin embarked boat  body  \\\n",
      "0      0   24160  211.3375       B5        S    2   NaN   \n",
      "1      2  113781  151.5500  C22 C26        S   11   NaN   \n",
      "2      2  113781  151.5500  C22 C26        S  NaN   NaN   \n",
      "\n",
      "                         home.dest  \n",
      "0                     St Louis, MO  \n",
      "1  Montreal, PQ / Chesterville, ON  \n",
      "2  Montreal, PQ / Chesterville, ON  \n",
      "\n",
      "============================================================\n",
      "DATASET: AI_ADOPTION\n",
      "============================================================\n",
      "üìÅ File: ai_adoption_dataset.csv\n",
      "üìä Shape: 145,000 rows √ó 9 columns\n",
      "üíæ Memory usage: 69.72 MB\n",
      "\n",
      "üìã Column Names:\n",
      "   1. country\n",
      "   2. industry\n",
      "   3. ai_tool\n",
      "   4. adoption_rate\n",
      "   5. daily_active_users\n",
      "   6. year\n",
      "   7. user_feedback\n",
      "   8. age_group\n",
      "   9. company_size\n",
      "\n",
      "üîç Data Types:\n",
      "object     6\n",
      "int64      2\n",
      "float64    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìà Sample Data (First 3 rows):\n",
      "     country        industry     ai_tool  adoption_rate  daily_active_users  \\\n",
      "0        USA      Technology     ChatGPT          40.45                2461   \n",
      "1     France   Manufacturing  Midjourney          35.72                8496   \n",
      "2  Australia  Transportation     ChatGPT          13.47                8641   \n",
      "\n",
      "   year                                      user_feedback age_group  \\\n",
      "0  2023  YyvLXOFyevRMSvJtkXodLvgejiqQNvSOZfeeJASDOVTxwm...     35-44   \n",
      "1  2024  AdFVhenjthYSKJNzxzfaGQk wLnjRCgrHTyfXQEFjFJCMO...     18-24   \n",
      "2  2024  zgNPmXBICRNbpjpTqIUWmMTeTYsInDNtAmzuxpDvcUZEAi...     45-54   \n",
      "\n",
      "  company_size  \n",
      "0      Startup  \n",
      "1   Enterprise  \n",
      "2      Startup  \n",
      "\n",
      "‚úÖ Dataset loading complete! Loaded 6 datasets successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define data path\n",
    "data_path = \"Datasource/\"\n",
    "\n",
    "# Dictionary to store all datasets\n",
    "datasets = {}\n",
    "\n",
    "# Function to safely load and inspect datasets\n",
    "def load_and_inspect_dataset(filename, dataset_name):\n",
    "    \"\"\"Load dataset and return basic information\"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        \n",
    "        # Load dataset based on file extension\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif filename.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif filename.endswith('.xls'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            return None, f\"Unsupported file format for {filename}\"\n",
    "        \n",
    "        # Store dataset\n",
    "        datasets[dataset_name] = df\n",
    "        \n",
    "        # Basic information\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DATASET: {dataset_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üìÅ File: {filename}\")\n",
    "        print(f\"üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        print(f\"\\nüìã Column Names:\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        \n",
    "        print(f\"\\nüîç Data Types:\")\n",
    "        print(df.dtypes.value_counts())\n",
    "        \n",
    "        print(f\"\\nüìà Sample Data (First 3 rows):\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "        return df, \"Success\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error loading {filename}: {str(e)}\"\n",
    "\n",
    "# Load all datasets\n",
    "print(\"üîÑ Loading all datasets...\")\n",
    "\n",
    "# 1. Airbnb Dataset\n",
    "result, message = load_and_inspect_dataset(\"airbnb.xlsx\", \"Airbnb\")\n",
    "if result is None:\n",
    "    print(f\"‚ùå {message}\")\n",
    "\n",
    "# 2. Netflix Dataset  \n",
    "result, message = load_and_inspect_dataset(\"netflix_titles.xlsx\", \"Netflix\")\n",
    "if result is None:\n",
    "    print(f\"‚ùå {message}\")\n",
    "\n",
    "# 3. Superstore Dataset\n",
    "result, message = load_and_inspect_dataset(\"sample_-_superstore.xls\", \"Superstore\")\n",
    "if result is None:\n",
    "    print(f\"‚ùå {message}\")\n",
    "\n",
    "# 4. Spotify Dataset\n",
    "result, message = load_and_inspect_dataset(\"SpotifyFeatures.csv\", \"Spotify\")\n",
    "if result is None:\n",
    "    print(f\"‚ùå {message}\")\n",
    "\n",
    "# 5. Titanic Dataset\n",
    "result, message = load_and_inspect_dataset(\"titanic passenger list.csv\", \"Titanic\")\n",
    "if result is None:\n",
    "    print(f\"‚ùå {message}\")\n",
    "\n",
    "# 6. AI Adoption Dataset\n",
    "result, message = load_and_inspect_dataset(\"ai_adoption_dataset.csv\", \"AI_Adoption\")\n",
    "if result is None:\n",
    "    print(f\"‚ùå {message}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loading complete! Loaded {len(datasets)} datasets successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1ba3b",
   "metadata": {},
   "source": [
    "## 3. Assess Data Quality (40% Weight)\n",
    "\n",
    "Now let's systematically evaluate the data quality of each dataset by examining:\n",
    "- **Missing data percentage** - How complete is the dataset?\n",
    "- **Data type consistency** - Are data types appropriate?\n",
    "- **Dataset size category** - Size implications for analysis\n",
    "- **General cleanliness** - Any obvious data issues?\n",
    "\n",
    "This assessment will form 40% of our final scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67005307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment Function\n",
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"Comprehensive data quality assessment\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç DATA QUALITY ASSESSMENT: {dataset_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Missing Data Analysis\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    missing_percentage = (missing_cells / total_cells) * 100\n",
    "    completeness = 100 - missing_percentage\n",
    "    \n",
    "    print(f\"üìä Missing Data Analysis:\")\n",
    "    print(f\"   ‚Ä¢ Total cells: {total_cells:,}\")\n",
    "    print(f\"   ‚Ä¢ Missing cells: {missing_cells:,}\")\n",
    "    print(f\"   ‚Ä¢ Completeness: {completeness:.1f}%\")\n",
    "    \n",
    "    # Missing data by column\n",
    "    missing_by_col = df.isnull().sum()\n",
    "    missing_cols = missing_by_col[missing_by_col > 0]\n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"   ‚Ä¢ Columns with missing data:\")\n",
    "        for col, count in missing_cols.head(5).items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"     - {col}: {count:,} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ No missing data found! ‚úÖ\")\n",
    "    \n",
    "    # 2. Data Types Analysis\n",
    "    print(f\"\\nüìã Data Types Analysis:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   ‚Ä¢ {dtype}: {count} columns\")\n",
    "    \n",
    "    # 3. Dataset Size Category\n",
    "    rows = df.shape[0]\n",
    "    if rows >= 50000:\n",
    "        size_category = \"Large (50K+ rows)\"\n",
    "        size_score = 4\n",
    "    elif rows >= 10000:\n",
    "        size_category = \"Medium (10K-50K rows)\"\n",
    "        size_score = 3\n",
    "    elif rows >= 1000:\n",
    "        size_category = \"Small (1K-10K rows)\"\n",
    "        size_score = 2\n",
    "    else:\n",
    "        size_category = \"Very Small (<1K rows)\"\n",
    "        size_score = 1\n",
    "    \n",
    "    print(f\"\\nüìè Dataset Size Category: {size_category}\")\n",
    "    \n",
    "    # 4. Basic Data Issues Check\n",
    "    print(f\"\\nüîç Basic Data Issues Check:\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    duplicate_pct = (duplicates / len(df)) * 100\n",
    "    print(f\"   ‚Ä¢ Duplicate rows: {duplicates:,} ({duplicate_pct:.1f}%)\")\n",
    "    \n",
    "    # Check numeric columns for obvious outliers\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"   ‚Ä¢ Numeric columns: {len(numeric_cols)}\")\n",
    "        # Quick outlier check on first numeric column\n",
    "        if len(numeric_cols) > 0:\n",
    "            col = numeric_cols[0]\n",
    "            col_data = df[col].dropna()\n",
    "            if len(col_data) > 0:\n",
    "                q1, q3 = col_data.quantile([0.25, 0.75])\n",
    "                iqr = q3 - q1\n",
    "                outliers = ((col_data < (q1 - 1.5 * iqr)) | (col_data > (q3 + 1.5 * iqr))).sum()\n",
    "                outlier_pct = (outliers / len(col_data)) * 100\n",
    "                print(f\"   ‚Ä¢ Outliers in '{col}': {outliers:,} ({outlier_pct:.1f}%)\")\n",
    "    \n",
    "    # 5. Calculate Quality Score\n",
    "    # Completeness score (0-40 points)\n",
    "    if completeness >= 95:\n",
    "        completeness_score = 40\n",
    "    elif completeness >= 85:\n",
    "        completeness_score = 30\n",
    "    elif completeness >= 70:\n",
    "        completeness_score = 20\n",
    "    else:\n",
    "        completeness_score = 10\n",
    "    \n",
    "    # Size score (0-10 points)\n",
    "    size_points = size_score * 2.5\n",
    "    \n",
    "    # Type consistency score (0-10 points) - simplified\n",
    "    type_score = min(10, len(numeric_cols) * 2)\n",
    "    \n",
    "    total_quality_score = completeness_score + size_points + type_score\n",
    "    \n",
    "    print(f\"\\nüéØ QUALITY SCORE BREAKDOWN:\")\n",
    "    print(f\"   ‚Ä¢ Completeness: {completeness_score}/40 points\")\n",
    "    print(f\"   ‚Ä¢ Dataset Size: {size_points}/10 points\")\n",
    "    print(f\"   ‚Ä¢ Type Diversity: {type_score}/10 points\")\n",
    "    print(f\"   ‚Ä¢ TOTAL QUALITY SCORE: {total_quality_score}/60 points\")\n",
    "    \n",
    "    return {\n",
    "        'completeness': completeness,\n",
    "        'missing_percentage': missing_percentage,\n",
    "        'size_category': size_category,\n",
    "        'size_score': size_score,\n",
    "        'duplicates': duplicates,\n",
    "        'numeric_columns': len(numeric_cols),\n",
    "        'quality_score': total_quality_score\n",
    "    }\n",
    "\n",
    "# Assess data quality for all datasets\n",
    "quality_results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    quality_results[name] = assess_data_quality(df, name)\n",
    "\n",
    "print(f\"\\n‚úÖ Data quality assessment complete for {len(datasets)} datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9f22d",
   "metadata": {},
   "source": [
    "## 4. Evaluate Business Relevance (35% Weight)\n",
    "\n",
    "Now let's evaluate the business relevance of each dataset by examining:\n",
    "- **Column names and business context** - What business questions can we answer?\n",
    "- **Revenue/cost impact potential** - Can this drive financial decisions?\n",
    "- **Executive interest level** - Would C-suite care about these insights?\n",
    "- **Real-world applicability** - How transferable are the skills and insights?\n",
    "\n",
    "This evaluation will form 35% of our final scoring and is crucial for portfolio impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f18389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Relevance Assessment Function\n",
    "def assess_business_relevance(df, dataset_name):\n",
    "    \"\"\"Evaluate business relevance and potential impact\"\"\"\n",
    "    \n",
    "    print(f\"\\nüíº BUSINESS RELEVANCE ASSESSMENT: {dataset_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Display all column names for manual business assessment\n",
    "    print(f\"üìã All Column Names ({len(df.columns)} total):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # Dataset-specific business analysis\n",
    "    potential_questions = []\n",
    "    revenue_impact = \"Medium\"\n",
    "    executive_interest = \"Medium\"\n",
    "    \n",
    "    if \"airbnb\" in dataset_name.lower():\n",
    "        potential_questions = [\n",
    "            \"What factors drive Airbnb pricing in different neighborhoods?\",\n",
    "            \"Which areas have the highest revenue potential for hosts?\", \n",
    "            \"How does property type affect occupancy and pricing?\",\n",
    "            \"What are the seasonal demand patterns?\",\n",
    "            \"Where should new hosts invest for maximum ROI?\"\n",
    "        ]\n",
    "        revenue_impact = \"High\"\n",
    "        executive_interest = \"High\"\n",
    "        business_score = 45\n",
    "        \n",
    "    elif \"superstore\" in dataset_name.lower():\n",
    "        potential_questions = [\n",
    "            \"Which products and regions drive the most profit?\",\n",
    "            \"What are the seasonal sales patterns?\",\n",
    "            \"How can we optimize inventory and reduce costs?\",\n",
    "            \"Which customer segments are most profitable?\",\n",
    "            \"What's the impact of discounts on profitability?\"\n",
    "        ]\n",
    "        revenue_impact = \"High\"\n",
    "        executive_interest = \"High\"\n",
    "        business_score = 42\n",
    "        \n",
    "    elif \"netflix\" in dataset_name.lower():\n",
    "        potential_questions = [\n",
    "            \"What content types perform best in different regions?\",\n",
    "            \"How has content strategy evolved over time?\",\n",
    "            \"What are the optimal content durations for engagement?\",\n",
    "            \"Which genres should be prioritized for new content?\",\n",
    "            \"How do release patterns affect viewership?\"\n",
    "        ]\n",
    "        revenue_impact = \"Medium\"\n",
    "        executive_interest = \"High\"\n",
    "        business_score = 35\n",
    "        \n",
    "    elif \"spotify\" in dataset_name.lower():\n",
    "        potential_questions = [\n",
    "            \"What audio features make songs popular?\",\n",
    "            \"How do musical preferences vary by genre?\",\n",
    "            \"Can we predict song popularity from audio features?\",\n",
    "            \"What's the evolution of music characteristics over time?\",\n",
    "            \"How can we improve recommendation algorithms?\"\n",
    "        ]\n",
    "        revenue_impact = \"Medium\"\n",
    "        executive_interest = \"Medium\"\n",
    "        business_score = 30\n",
    "        \n",
    "    elif \"titanic\" in dataset_name.lower():\n",
    "        potential_questions = [\n",
    "            \"What factors influenced survival rates?\",\n",
    "            \"How did social class affect survival chances?\",\n",
    "            \"What can we learn about emergency response?\",\n",
    "            \"How did demographics influence outcomes?\",\n",
    "            \"What lessons apply to modern safety protocols?\"\n",
    "        ]\n",
    "        revenue_impact = \"Low\"\n",
    "        executive_interest = \"Low\"\n",
    "        business_score = 15\n",
    "        \n",
    "    elif \"ai\" in dataset_name.lower():\n",
    "        potential_questions = [\n",
    "            \"Which industries are leading AI adoption?\",\n",
    "            \"What factors drive successful AI implementation?\",\n",
    "            \"How do adoption rates vary by company size?\",\n",
    "            \"What are the key barriers to AI adoption?\",\n",
    "            \"Which AI tools show the highest ROI?\"\n",
    "        ]\n",
    "        revenue_impact = \"High\"\n",
    "        executive_interest = \"High\"\n",
    "        business_score = 40\n",
    "        \n",
    "    else:\n",
    "        potential_questions = [\"General analytical questions possible\"]\n",
    "        business_score = 25\n",
    "    \n",
    "    print(f\"\\n‚ùì Potential Business Questions:\")\n",
    "    for i, question in enumerate(potential_questions, 1):\n",
    "        print(f\"   {i}. {question}\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Revenue/Cost Impact: {revenue_impact}\")\n",
    "    print(f\"üëî Executive Interest Level: {executive_interest}\")\n",
    "    print(f\"üéØ Business Relevance Score: {business_score}/50 points\")\n",
    "    \n",
    "    return {\n",
    "        'potential_questions': potential_questions,\n",
    "        'revenue_impact': revenue_impact,\n",
    "        'executive_interest': executive_interest,\n",
    "        'business_score': business_score\n",
    "    }\n",
    "\n",
    "# Assess business relevance for all datasets\n",
    "business_results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    business_results[name] = assess_business_relevance(df, name)\n",
    "\n",
    "print(f\"\\n‚úÖ Business relevance assessment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1c0f7",
   "metadata": {},
   "source": [
    "## 5. Evaluate Technical Complexity (25% Weight)\n",
    "\n",
    "Let's assess the technical complexity and skill showcase potential of each dataset by examining:\n",
    "- **Statistical analysis opportunities** - Correlations, trends, hypothesis testing\n",
    "- **Visualization potential** - Geographic, time-series, interactive charts\n",
    "- **Machine learning applications** - Predictive models, clustering, classification\n",
    "- **Advanced analytics potential** - Feature engineering, segmentation, forecasting\n",
    "\n",
    "This evaluation will form 25% of our final scoring and determines how well we can showcase advanced skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3447f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical Complexity Assessment Function\n",
    "def assess_technical_complexity(df, dataset_name):\n",
    "    \"\"\"Evaluate technical complexity and skill showcase potential\"\"\"\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è TECHNICAL COMPLEXITY ASSESSMENT: {dataset_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Basic data characteristics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    datetime_cols = df.select_dtypes(include=['datetime']).columns\n",
    "    \n",
    "    print(f\"üìä Data Type Breakdown:\")\n",
    "    print(f\"   ‚Ä¢ Numeric columns: {len(numeric_cols)}\")\n",
    "    print(f\"   ‚Ä¢ Categorical columns: {len(categorical_cols)}\")\n",
    "    print(f\"   ‚Ä¢ DateTime columns: {len(datetime_cols)}\")\n",
    "    \n",
    "    # Statistical Analysis Opportunities\n",
    "    stats_score = 0\n",
    "    stats_opportunities = []\n",
    "    \n",
    "    if len(numeric_cols) >= 3:\n",
    "        stats_opportunities.append(\"Correlation analysis between multiple variables\")\n",
    "        stats_score += 5\n",
    "    if len(numeric_cols) >= 2:\n",
    "        stats_opportunities.append(\"Comparative analysis and statistical testing\")\n",
    "        stats_score += 3\n",
    "    if len(categorical_cols) >= 2:\n",
    "        stats_opportunities.append(\"Segmentation and group analysis\")\n",
    "        stats_score += 4\n",
    "    if len(datetime_cols) >= 1:\n",
    "        stats_opportunities.append(\"Time series analysis and trend identification\")\n",
    "        stats_score += 5\n",
    "    if df.shape[0] > 1000:\n",
    "        stats_opportunities.append(\"Robust statistical inference\")\n",
    "        stats_score += 3\n",
    "    \n",
    "    print(f\"\\nüìà Statistical Analysis Opportunities:\")\n",
    "    for i, opp in enumerate(stats_opportunities, 1):\n",
    "        print(f\"   {i}. {opp}\")\n",
    "    \n",
    "    # Visualization Potential\n",
    "    viz_score = 0\n",
    "    viz_opportunities = []\n",
    "    \n",
    "    # Check for geographic potential\n",
    "    geo_cols = [col for col in df.columns if any(geo_word in col.lower() \n",
    "                for geo_word in ['lat', 'lon', 'latitude', 'longitude', 'location', 'city', 'state', 'country', 'region', 'neighborhood'])]\n",
    "    if geo_cols:\n",
    "        viz_opportunities.append(\"Geographic visualization and mapping\")\n",
    "        viz_score += 8\n",
    "    \n",
    "    if len(datetime_cols) >= 1:\n",
    "        viz_opportunities.append(\"Time series plots and trend visualization\")\n",
    "        viz_score += 6\n",
    "    if len(numeric_cols) >= 2:\n",
    "        viz_opportunities.append(\"Scatter plots and correlation matrices\")\n",
    "        viz_score += 4\n",
    "    if len(categorical_cols) >= 1:\n",
    "        viz_opportunities.append(\"Bar charts and distribution analysis\")\n",
    "        viz_score += 3\n",
    "    if df.shape[0] > 10000:\n",
    "        viz_opportunities.append(\"Interactive dashboards with plotly\")\n",
    "        viz_score += 5\n",
    "    \n",
    "    print(f\"\\nüìä Visualization Potential:\")\n",
    "    for i, opp in enumerate(viz_opportunities, 1):\n",
    "        print(f\"   {i}. {opp}\")\n",
    "    \n",
    "    # Machine Learning Applications\n",
    "    ml_score = 0\n",
    "    ml_opportunities = []\n",
    "    \n",
    "    if len(numeric_cols) >= 3:\n",
    "        ml_opportunities.append(\"Regression modeling for prediction\")\n",
    "        ml_score += 5\n",
    "    if len(categorical_cols) >= 1 and len(numeric_cols) >= 2:\n",
    "        ml_opportunities.append(\"Classification algorithms\")\n",
    "        ml_score += 5\n",
    "    if len(numeric_cols) >= 4:\n",
    "        ml_opportunities.append(\"Clustering analysis and segmentation\")\n",
    "        ml_score += 4\n",
    "    if df.shape[0] > 5000:\n",
    "        ml_opportunities.append(\"Feature engineering and selection\")\n",
    "        ml_score += 3\n",
    "    if len(datetime_cols) >= 1:\n",
    "        ml_opportunities.append(\"Time series forecasting\")\n",
    "        ml_score += 4\n",
    "    \n",
    "    # Dataset-specific ML opportunities\n",
    "    if \"price\" in str(df.columns).lower() or \"cost\" in str(df.columns).lower():\n",
    "        ml_opportunities.append(\"Pricing optimization models\")\n",
    "        ml_score += 4\n",
    "    if \"rating\" in str(df.columns).lower() or \"score\" in str(df.columns).lower():\n",
    "        ml_opportunities.append(\"Recommendation systems\")\n",
    "        ml_score += 3\n",
    "    \n",
    "    print(f\"\\nü§ñ Machine Learning Applications:\")\n",
    "    for i, opp in enumerate(ml_opportunities, 1):\n",
    "        print(f\"   {i}. {opp}\")\n",
    "    \n",
    "    # Calculate total technical score\n",
    "    total_tech_score = min(25, stats_score + viz_score + ml_score)\n",
    "    \n",
    "    print(f\"\\nüéØ TECHNICAL COMPLEXITY SCORE BREAKDOWN:\")\n",
    "    print(f\"   ‚Ä¢ Statistical Analysis: {min(10, stats_score)}/10 points\")\n",
    "    print(f\"   ‚Ä¢ Visualization Potential: {min(10, viz_score)}/10 points\")\n",
    "    print(f\"   ‚Ä¢ ML Applications: {min(5, ml_score)}/5 points\")\n",
    "    print(f\"   ‚Ä¢ TOTAL TECHNICAL SCORE: {total_tech_score}/25 points\")\n",
    "    \n",
    "    return {\n",
    "        'numeric_columns': len(numeric_cols),\n",
    "        'categorical_columns': len(categorical_cols),\n",
    "        'datetime_columns': len(datetime_cols),\n",
    "        'geographic_potential': len(geo_cols) > 0,\n",
    "        'stats_opportunities': stats_opportunities,\n",
    "        'viz_opportunities': viz_opportunities,\n",
    "        'ml_opportunities': ml_opportunities,\n",
    "        'technical_score': total_tech_score\n",
    "    }\n",
    "\n",
    "# Assess technical complexity for all datasets\n",
    "technical_results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    technical_results[name] = assess_technical_complexity(df, name)\n",
    "\n",
    "print(f\"\\n‚úÖ Technical complexity assessment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffcb6a",
   "metadata": {},
   "source": [
    "## 6. Score and Rank Datasets\n",
    "\n",
    "Now let's combine all our assessments into a final scoring matrix using our weighted criteria:\n",
    "- **Data Quality**: 40% weight\n",
    "- **Business Relevance**: 35% weight  \n",
    "- **Technical Complexity**: 25% weight\n",
    "\n",
    "This will give us an objective ranking to guide our dataset selection decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive scoring matrix\n",
    "def create_scoring_matrix():\n",
    "    \"\"\"Combine all assessments into final scoring matrix\"\"\"\n",
    "    \n",
    "    print(\"üèÜ FINAL DATASET SCORING MATRIX\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Weights\n",
    "    weights = {\n",
    "        'quality': 0.40,    # 40%\n",
    "        'business': 0.35,   # 35%\n",
    "        'technical': 0.25   # 25%\n",
    "    }\n",
    "    \n",
    "    # Create scoring DataFrame\n",
    "    scoring_data = []\n",
    "    \n",
    "    for name in datasets.keys():\n",
    "        # Normalize scores to 0-100 scale\n",
    "        quality_score = (quality_results[name]['quality_score'] / 60) * 100  # Max was 60\n",
    "        business_score = (business_results[name]['business_score'] / 50) * 100  # Max was 50\n",
    "        technical_score = (technical_results[name]['technical_score'] / 25) * 100  # Max was 25\n",
    "        \n",
    "        # Calculate weighted final score\n",
    "        final_score = (\n",
    "            quality_score * weights['quality'] +\n",
    "            business_score * weights['business'] +\n",
    "            technical_score * weights['technical']\n",
    "        )\n",
    "        \n",
    "        scoring_data.append({\n",
    "            'Dataset': name,\n",
    "            'Data_Quality_Score': quality_score,\n",
    "            'Business_Score': business_score,\n",
    "            'Technical_Score': technical_score,\n",
    "            'Final_Score': final_score,\n",
    "            'Size': f\"{datasets[name].shape[0]:,} √ó {datasets[name].shape[1]}\",\n",
    "            'Completeness': f\"{quality_results[name]['completeness']:.1f}%\"\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and sort by final score\n",
    "    df_scores = pd.DataFrame(scoring_data)\n",
    "    df_scores = df_scores.sort_values('Final_Score', ascending=False).reset_index(drop=True)\n",
    "    df_scores['Rank'] = range(1, len(df_scores) + 1)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nSCORING BREAKDOWN:\")\n",
    "    print(f\"{'Rank':<5} {'Dataset':<12} {'Quality':<8} {'Business':<9} {'Technical':<9} {'Final':<8} {'Size':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for _, row in df_scores.iterrows():\n",
    "        print(f\"{row['Rank']:<5} {row['Dataset']:<12} {row['Data_Quality_Score']:<8.1f} \"\n",
    "              f\"{row['Business_Score']:<9.1f} {row['Technical_Score']:<9.1f} \"\n",
    "              f\"{row['Final_Score']:<8.1f} {row['Size']:<15}\")\n",
    "    \n",
    "    print(f\"\\nWEIGHTING APPLIED:\")\n",
    "    print(f\"‚Ä¢ Data Quality: {weights['quality']*100:.0f}%\")\n",
    "    print(f\"‚Ä¢ Business Relevance: {weights['business']*100:.0f}%\") \n",
    "    print(f\"‚Ä¢ Technical Complexity: {weights['technical']*100:.0f}%\")\n",
    "    \n",
    "    return df_scores\n",
    "\n",
    "# Generate final scoring matrix\n",
    "final_scores = create_scoring_matrix()\n",
    "\n",
    "# Display detailed results for top 3\n",
    "print(f\"\\nü•á TOP 3 DATASETS DETAILED BREAKDOWN:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(min(3, len(final_scores))):\n",
    "    row = final_scores.iloc[i]\n",
    "    name = row['Dataset']\n",
    "    \n",
    "    print(f\"\\n{i+1}. {name.upper()} - Final Score: {row['Final_Score']:.1f}\")\n",
    "    print(f\"   üìä Size: {row['Size']}\")\n",
    "    print(f\"   ‚úÖ Completeness: {row['Completeness']}\")\n",
    "    print(f\"   üíº Business Questions: {len(business_results[name]['potential_questions'])}\")\n",
    "    print(f\"   ‚öôÔ∏è ML Opportunities: {len(technical_results[name]['ml_opportunities'])}\")\n",
    "    print(f\"   üìà Viz Opportunities: {len(technical_results[name]['viz_opportunities'])}\")\n",
    "\n",
    "winner = final_scores.iloc[0]['Dataset']\n",
    "winner_score = final_scores.iloc[0]['Final_Score']\n",
    "\n",
    "print(f\"\\nüèÜ WINNER: {winner.upper()}\")\n",
    "print(f\"üìä Score: {winner_score:.1f}/100\")\n",
    "print(f\"üéØ This dataset offers the best combination of data quality, business relevance, and technical complexity!\")\n",
    "\n",
    "# Store winner for next section\n",
    "selected_dataset = winner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485323b",
   "metadata": {},
   "source": [
    "## 7. Document Selection Rationale\n",
    "\n",
    "Based on our systematic evaluation, let's document the final selection rationale and expected project outcomes. This justification will be crucial for demonstrating our strategic decision-making process to potential employers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f82521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document final selection rationale\n",
    "def document_selection_rationale(selected_dataset, final_scores):\n",
    "    \"\"\"Create comprehensive selection justification\"\"\"\n",
    "    \n",
    "    print(\"üìù DATASET SELECTION RATIONALE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get selected dataset info\n",
    "    winner_row = final_scores[final_scores['Dataset'] == selected_dataset].iloc[0]\n",
    "    winner_name = selected_dataset\n",
    "    winner_score = winner_row['Final_Score']\n",
    "    \n",
    "    print(f\"\\nüèÜ SELECTED DATASET: {winner_name.upper()}\")\n",
    "    print(f\"üìä Final Score: {winner_score:.1f}/100\")\n",
    "    print(f\"üìè Dataset Size: {winner_row['Size']}\")\n",
    "    print(f\"‚úÖ Data Completeness: {winner_row['Completeness']}\")\n",
    "    \n",
    "    # Detailed justification\n",
    "    quality_info = quality_results[winner_name]\n",
    "    business_info = business_results[winner_name]\n",
    "    technical_info = technical_results[winner_name]\n",
    "    \n",
    "    print(f\"\\nüìä WHY THIS DATASET WAS SELECTED:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\n1. üéØ DATA QUALITY EXCELLENCE:\")\n",
    "    print(f\"   ‚Ä¢ {quality_info['completeness']:.1f}% data completeness\")\n",
    "    print(f\"   ‚Ä¢ {quality_info['size_category']}\")\n",
    "    print(f\"   ‚Ä¢ {quality_info['numeric_columns']} numeric columns for analysis\")\n",
    "    print(f\"   ‚Ä¢ Quality Score: {winner_row['Data_Quality_Score']:.1f}/100\")\n",
    "    \n",
    "    print(f\"\\n2. üíº STRONG BUSINESS RELEVANCE:\")\n",
    "    print(f\"   ‚Ä¢ Revenue Impact: {business_info['revenue_impact']}\")\n",
    "    print(f\"   ‚Ä¢ Executive Interest: {business_info['executive_interest']}\")\n",
    "    print(f\"   ‚Ä¢ {len(business_info['potential_questions'])} key business questions identified\")\n",
    "    print(f\"   ‚Ä¢ Business Score: {winner_row['Business_Score']:.1f}/100\")\n",
    "    \n",
    "    print(f\"\\n3. ‚öôÔ∏è RICH TECHNICAL COMPLEXITY:\")\n",
    "    print(f\"   ‚Ä¢ {len(technical_info['stats_opportunities'])} statistical analysis opportunities\")\n",
    "    print(f\"   ‚Ä¢ {len(technical_info['viz_opportunities'])} visualization possibilities\")\n",
    "    print(f\"   ‚Ä¢ {len(technical_info['ml_opportunities'])} machine learning applications\")\n",
    "    print(f\"   ‚Ä¢ Technical Score: {winner_row['Technical_Score']:.1f}/100\")\n",
    "    \n",
    "    # Key business questions\n",
    "    print(f\"\\n‚ùì KEY BUSINESS QUESTIONS TO ADDRESS:\")\n",
    "    for i, question in enumerate(business_info['potential_questions'][:5], 1):\n",
    "        print(f\"   {i}. {question}\")\n",
    "    \n",
    "    # Skills to demonstrate\n",
    "    print(f\"\\nüõ†Ô∏è SKILLS TO DEMONSTRATE:\")\n",
    "    all_skills = []\n",
    "    \n",
    "    # From statistical opportunities\n",
    "    if technical_info['stats_opportunities']:\n",
    "        all_skills.extend([\"Statistical Analysis\", \"Correlation Studies\", \"Hypothesis Testing\"])\n",
    "    \n",
    "    # From visualization opportunities  \n",
    "    if technical_info['viz_opportunities']:\n",
    "        all_skills.extend([\"Data Visualization\", \"Dashboard Creation\"])\n",
    "        if technical_info['geographic_potential']:\n",
    "            all_skills.append(\"Geographic Analysis\")\n",
    "    \n",
    "    # From ML opportunities\n",
    "    if technical_info['ml_opportunities']:\n",
    "        all_skills.extend([\"Machine Learning\", \"Predictive Modeling\"])\n",
    "    \n",
    "    # Business skills\n",
    "    all_skills.extend([\"Business Intelligence\", \"Strategic Insights\", \"Executive Communication\"])\n",
    "    \n",
    "    unique_skills = list(set(all_skills))\n",
    "    for i, skill in enumerate(unique_skills[:8], 1):\n",
    "        print(f\"   {i}. {skill}\")\n",
    "    \n",
    "    # Alternative considerations\n",
    "    print(f\"\\nü§î ALTERNATIVE CONSIDERATIONS:\")\n",
    "    second_place = final_scores.iloc[1]\n",
    "    print(f\"   ‚Ä¢ Second Choice: {second_place['Dataset']} (Score: {second_place['Final_Score']:.1f})\")\n",
    "    print(f\"   ‚Ä¢ Why not selected: Lower overall weighted score\")\n",
    "    \n",
    "    eliminated = final_scores.iloc[-1]\n",
    "    print(f\"   ‚Ä¢ Lowest Ranked: {eliminated['Dataset']} (Score: {eliminated['Final_Score']:.1f})\")\n",
    "    print(f\"   ‚Ä¢ Main weakness: Limited business applicability or data quality issues\")\n",
    "    \n",
    "    # Expected outcomes\n",
    "    print(f\"\\nüéØ EXPECTED PROJECT OUTCOMES:\")\n",
    "    print(f\"   1. Technical Skills: Advanced Python, pandas, visualization, ML\")\n",
    "    print(f\"   2. Business Insights: Revenue optimization, market analysis\") \n",
    "    print(f\"   3. Portfolio Impact: Senior-level strategic thinking demonstration\")\n",
    "    print(f\"   4. Employer Value: Real-world applicable skills and insights\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ SELECTION COMPLETE!\")\n",
    "    print(f\"Ready to proceed with comprehensive analysis of {winner_name} dataset.\")\n",
    "    \n",
    "    return {\n",
    "        'selected_dataset': winner_name,\n",
    "        'final_score': winner_score,\n",
    "        'justification_complete': True\n",
    "    }\n",
    "\n",
    "# Document the selection rationale\n",
    "selection_rationale = document_selection_rationale(selected_dataset, final_scores)\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"1. Create comprehensive analysis notebook for {selected_dataset}\")\n",
    "print(f\"2. Update Dataset_Selection_Methodology.md with results\")\n",
    "print(f\"3. Begin Phase 2: Comprehensive Analysis\")\n",
    "print(f\"4. Develop executive-ready insights and recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
